\section{Machine Learning}

The importance of studying machine learning comes from the complexity of
the data-driven world in which we currently live. With the explosion of
the Internet, the social networks and the spread of an ubiquitous model of
connectivity, more and more complex data need to be analyzed: from web
traffic statistics and electronic commerce data, to digital collections of
images, video, music, and text.

Our INCA group has made contributions in several machine learning
sub-fields:
\begin{itemize}
\item \textbf{Ensemble Learning:} Multiple models, such as classifiers or
regressors, are combined to solve a machine learning problem. In other
words, ensembles combine a set of predictors to hopefully form a stronger
hypothesis.
Some research publications in this fields are \cite{NanculefVAM06},
\cite{ValleSAMF10}, \cite{FernandezVSA12} and \cite{NanculefVAM12}.
Specifically for unbalanced data, we have proposed two ensemble techniques with
asymmetric cost: \cite{NanculefVAM07} and \cite{OrmenoRVAA12}
\item \textbf{Text Processing:} Massive use of the Internet worldwide has
increased the number of domains in which text data is generated on large
amounts. Social network analysis and online collection selection are two
examples in which knowledge extraction over large text collections needs
to be performed. Text representations, information retrieval models and
pattern recognition techniques traditionally employed are not designed to
work under low memory and single-pass-over-each-instance constraints.
Consequently, we are aimed at building new text representations and to
create and extend current clustering algorithms to deal with high
dimensional and massive text data under the abovementioned contraints.

Some publications related to the applications of machine learning
techniques for text processing and information retrieval are \cite{ZMA14}
and \cite{MZ09}. Finally, \cite{ZGDAVS13} and \cite{CSHZ12} are two
examples of our work in the field social network analysis.

\item \textbf{Distributed Learning:}
When there are large data sources that are distributed geographically, it
is infeasible to centralize them in order to infer a model from them, due
to the transmission cost and memory restrictions. Another problem that is
inherent to Distributed Learning, arises from the different underlying
laws of probability that these data sources may have. Treating this data
sources as if there came from a single virtual table, could lead to
generalization problems. To solve this, we have proposed methods to build
neighborhoods of similar data sources, and then with ensemble methods
build a general model that is able give better results than other state of
the art methods. We have mainly dealt with the regression task
\cite{Allende13}, \cite{Allende14}. 

\end{itemize}
